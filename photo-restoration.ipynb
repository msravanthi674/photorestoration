{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10627779,"sourceType":"datasetVersion","datasetId":6580233}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1: Import required libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nimport os\nfrom PIL import Image\nimport numpy as np\nfrom tqdm.notebook import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T06:31:46.775772Z","iopub.execute_input":"2025-02-01T06:31:46.775978Z","iopub.status.idle":"2025-02-01T06:31:52.290800Z","shell.execute_reply.started":"2025-02-01T06:31:46.775935Z","shell.execute_reply":"2025-02-01T06:31:52.289911Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# 2: Define the Generator Network","metadata":{}},{"cell_type":"code","source":"class ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(channels)\n        self.prelu = nn.PReLU()\n        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(channels)\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.prelu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out += residual\n        return out\n\nclass Generator(nn.Module):\n    def __init__(self, num_residual_blocks=16):\n        super(Generator, self).__init__()\n        \n        # First conv layer\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=9, padding=4),\n            nn.PReLU()\n        )\n        \n        # Residual blocks\n        res_blocks = []\n        for _ in range(num_residual_blocks):\n            res_blocks.append(ResidualBlock(64))\n        self.res_blocks = nn.Sequential(*res_blocks)\n        \n        # Second conv layer\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64)\n        )\n        \n        # Upsampling layers\n        self.upsampling = nn.Sequential(\n            nn.Conv2d(64, 256, kernel_size=3, padding=1),\n            nn.PixelShuffle(2),\n            nn.PReLU(),\n            nn.Conv2d(64, 256, kernel_size=3, padding=1),\n            nn.PixelShuffle(2),\n            nn.PReLU()\n        )\n        \n        # Final output layer\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(64, 3, kernel_size=9, padding=4),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        out1 = self.conv1(x)\n        out = self.res_blocks(out1)\n        out2 = self.conv2(out)\n        out = torch.add(out1, out2)\n        out = self.upsampling(out)\n        out = self.conv3(out)\n        return out\n\n    def save_checkpoint(self, epoch, optimizer, loss, filename=\"generator_checkpoint.pth\"):\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': self.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': loss,\n        }\n        torch.save(checkpoint, filename)\n        print(f\"Checkpoint saved: {filename}\")\n\n    def load_checkpoint(self, filename=\"generator_checkpoint.pth\", optimizer=None):\n        if os.path.exists(filename):\n            checkpoint = torch.load(filename)\n            self.load_state_dict(checkpoint['model_state_dict'])\n            if optimizer is not None:\n                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            epoch = checkpoint['epoch']\n            loss = checkpoint['loss']\n            print(f\"Checkpoint loaded: {filename}\")\n            return epoch, loss\n        else:\n            print(f\"No checkpoint found at {filename}\")\n            return 0, None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T06:31:52.292085Z","iopub.execute_input":"2025-02-01T06:31:52.292392Z","iopub.status.idle":"2025-02-01T06:31:52.302886Z","shell.execute_reply.started":"2025-02-01T06:31:52.292372Z","shell.execute_reply":"2025-02-01T06:31:52.302023Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# 3: Define the Discriminator Network","metadata":{}},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.LeakyReLU(0.2),\n            \n            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2),\n            \n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2),\n            \n            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2),\n            \n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2),\n            \n            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2),\n            \n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(256, 1024, kernel_size=1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(1024, 1, kernel_size=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.net(x).view(-1, 1).squeeze(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T06:31:57.140621Z","iopub.execute_input":"2025-02-01T06:31:57.140908Z","iopub.status.idle":"2025-02-01T06:31:57.147361Z","shell.execute_reply.started":"2025-02-01T06:31:57.140887Z","shell.execute_reply":"2025-02-01T06:31:57.146356Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# 4: Custom Dataset Class","metadata":{}},{"cell_type":"code","source":"class OldPhotoDataset(Dataset):\n    def __init__(self, hr_dir, hr_size=256, transform=None):\n        self.hr_dir = hr_dir\n        self.hr_size = hr_size\n        self.transform = transform\n        self.image_files = os.listdir(hr_dir)\n        \n    def __len__(self):\n        return len(self.image_files)\n    \n    def __getitem__(self, idx):\n        img_name = self.image_files[idx]\n        hr_image = Image.open(os.path.join(self.hr_dir, img_name))\n        \n        # Resize HR image to fixed size\n        hr_image = hr_image.resize((self.hr_size, self.hr_size), Image.BICUBIC)\n        \n        # Create low-res version\n        lr_image = hr_image.resize((self.hr_size//4, self.hr_size//4), Image.BICUBIC)\n        \n        if self.transform:\n            hr_image = self.transform(hr_image)\n            lr_image = self.transform(lr_image)\n            \n        return lr_image, hr_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T06:34:26.141233Z","iopub.execute_input":"2025-02-01T06:34:26.141566Z","iopub.status.idle":"2025-02-01T06:34:26.147935Z","shell.execute_reply.started":"2025-02-01T06:34:26.141536Z","shell.execute_reply":"2025-02-01T06:34:26.146958Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# 5: Training Function","metadata":{}},{"cell_type":"code","source":"def train_model(generator, discriminator, train_loader, num_epochs, device, checkpoint_interval=5):\n    criterion_GAN = nn.BCELoss()\n    criterion_content = nn.MSELoss()\n    \n    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002)\n    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002)\n    \n    # Load checkpoint if exists\n    start_epoch, _ = generator.load_checkpoint(optimizer=optimizer_G)\n    \n    for epoch in range(start_epoch, num_epochs):\n        running_g_loss = 0.0\n        for i, (lr_images, hr_images) in enumerate(tqdm(train_loader)):\n            batch_size = lr_images.size(0)\n            real_label = torch.ones(batch_size).to(device)\n            fake_label = torch.zeros(batch_size).to(device)\n            \n            lr_images = lr_images.to(device)\n            hr_images = hr_images.to(device)\n            \n            # Train Discriminator\n            optimizer_D.zero_grad()\n            sr_images = generator(lr_images)\n            \n            real_output = discriminator(hr_images)\n            fake_output = discriminator(sr_images.detach())\n            \n            d_loss_real = criterion_GAN(real_output, real_label)\n            d_loss_fake = criterion_GAN(fake_output, fake_label)\n            d_loss = d_loss_real + d_loss_fake\n            \n            d_loss.backward()\n            optimizer_D.step()\n            \n            # Train Generator\n            optimizer_G.zero_grad()\n            \n            fake_output = discriminator(sr_images)\n            content_loss = criterion_content(sr_images, hr_images)\n            adversarial_loss = criterion_GAN(fake_output, real_label)\n            \n            g_loss = content_loss + 0.001 * adversarial_loss\n            running_g_loss += g_loss.item()\n            \n            g_loss.backward()\n            optimizer_G.step()\n            \n            if i % 100 == 0:\n                print(f'Epoch [{epoch}/{num_epochs}], Step [{i}/{len(train_loader)}], '\n                      f'D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}')\n        \n        # Save checkpoint at interval\n        if (epoch + 1) % checkpoint_interval == 0:\n            generator.save_checkpoint(\n                epoch + 1,\n                optimizer_G,\n                running_g_loss / len(train_loader),\n                f\"generator_checkpoint_epoch_{epoch+1}.pth\"\n            )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T06:34:29.235425Z","iopub.execute_input":"2025-02-01T06:34:29.235713Z","iopub.status.idle":"2025-02-01T06:34:29.243662Z","shell.execute_reply.started":"2025-02-01T06:34:29.235691Z","shell.execute_reply":"2025-02-01T06:34:29.242900Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# 6: Setup and Training","metadata":{}},{"cell_type":"code","source":"# Initialize device and models\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Transform for the images\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Create the dataset and dataloader\ndataset = OldPhotoDataset(\n    hr_dir='/kaggle/input/photos/DIV2K_train_HR',\n    hr_size=256,  # Set fixed size for high-resolution images\n    transform=transform\n)\ntrain_loader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)  # Removed collate_fn since we don't need it anymore","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T06:34:56.231655Z","iopub.execute_input":"2025-02-01T06:34:56.231937Z","iopub.status.idle":"2025-02-01T06:34:56.239744Z","shell.execute_reply.started":"2025-02-01T06:34:56.231915Z","shell.execute_reply":"2025-02-01T06:34:56.239046Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# 7: Evaluation Function","metadata":{}},{"cell_type":"code","source":"def evaluate_model(generator, val_loader, device):\n    generator.eval()\n    with torch.no_grad():\n        for i, (lr_img, hr_img) in enumerate(val_loader):\n            lr_img = lr_img.to(device)\n            sr_img = generator(lr_img)\n            \n            # Save the super-resolved image\n            save_image(sr_img, f'super_resolved_image_{i}.png')\n            save_image(hr_img, f'high_res_image_{i}.png')\n            \n            if i >= 5:  # Save first 6 images\n                break\n    print(\"Evaluation complete. Check the saved images.\")\n\n# To evaluate after training\nevaluate_model(generator, train_loader, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T06:35:07.247826Z","iopub.execute_input":"2025-02-01T06:35:07.248149Z","iopub.status.idle":"2025-02-01T06:35:11.349912Z","shell.execute_reply.started":"2025-02-01T06:35:07.248120Z","shell.execute_reply":"2025-02-01T06:35:11.348933Z"}},"outputs":[{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7a40c72c5bd0>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n    if w.is_alive():\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\nAssertionErrorException ignored in: : <function _MultiProcessingDataLoaderIter.__del__ at 0x7a40c72c5bd0>can only test a child process\n\nTraceback (most recent call last):\nException ignored in:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n    <function _MultiProcessingDataLoaderIter.__del__ at 0x7a40c72c5bd0>\nself._shutdown_workers()Traceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n      File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\nself._shutdown_workers()    \n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n    if w.is_alive():if w.is_alive():\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n    \n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\nassert self._parent_pid == os.getpid(), 'can only test a child process'\n    AssertionErrorassert self._parent_pid == os.getpid(), 'can only test a child process': \ncan only test a child processAssertionError: \ncan only test a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7a40c72c5bd0>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n    if w.is_alive():\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"Evaluation complete. Check the saved images.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}